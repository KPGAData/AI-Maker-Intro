{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the OpenAI Library to Programmatically Access GPT-3.5-turbo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai cohere tiktoken -q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API key\")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First Prompt\n",
    "\n",
    "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) if you get stuck!\n",
    "\n",
    "Let's create a `ChatCompletion` model to kick things off!\n",
    "\n",
    "There are three \"roles\" available to use:\n",
    "\n",
    "- `system`\n",
    "- `assistant`\n",
    "- `user`\n",
    "\n",
    "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide)\n",
    "\n",
    "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
    "\n",
    "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9LFTUDSgOINy3a1NDLTG9kD5S5R4Y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two different projects in the cryptocurrency and blockchain space.\\n\\n1. LangChain: LangChain is a project that focuses on solving the challenge of language barriers in the blockchain and cryptocurrency industry. The platform aims to provide language translation services for various blockchain projects, making them more accessible to a global audience. LangChain utilizes blockchain technology to ensure the accuracy and security of translations, and it also offers additional services such as language tutoring and community support.\\n\\n2. LlamaIndex: LlamaIndex, on the other hand, is a platform that provides users with real-time market data and analysis for various cryptocurrencies and blockchain projects. It offers insights, charts, and statistics to help users make informed investment decisions in the cryptocurrency market. LlamaIndex aggregates data from various sources and presents it in a user-friendly format, allowing users to track market trends and monitor the performance of their investments.\\n\\nIn summary, LangChain focuses on language translation services for the blockchain industry, while LlamaIndex provides market data and analysis for cryptocurrencies. Both projects play different roles in the ecosystem and cater to different needs of users in the blockchain and cryptocurrency space.', role='assistant', function_call=None, tool_calls=None))], created=1714851400, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3b956da36b', usage=CompletionUsage(completion_tokens=228, prompt_tokens=19, total_tokens=247))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":YOUR_PROMPT}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
    "\n",
    "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model:str = \"gpt-3.5-turbo\") -> dict:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def user_prompt(message: str) -> dict: \n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def system_prompt(message: str) -> dict: \n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict: \n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "One cyber security stock that fits this description is CrowdStrike Holdings Inc. (CRWD). CrowdStrike is a fast-growing company that provides cloud-delivered endpoint protection and threat intelligence services. The company's stock price has risen significantly since its IPO in 2019, but many analysts believe that it still has significant growth potential in the rapidly expanding cyber security market. With a market capitalization of over $50 billion and a leadership position in cloud-based security solutions, CrowdStrike is considered a top pick for investors looking to capitalize on the growing demand for cyber security services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LANG_LLAMA_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "EARTH_SUN_PROMPT = \"What is the distance between earth and sun in miles?\"\n",
    "BLACK_HOLE_PROMPT = \"What is the closest black hole to us?\"\n",
    "STOCK_PROMPT = \"What is a cyber security stock that is valued less than it's growth potential?\"\n",
    "\n",
    "messages = [user_prompt(STOCK_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on extending this a bit, and incorporate a `system` message as well!\n",
    "\n",
    "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
    "\n",
    ">REMINDER: The system message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have a preference for ice right now because I am absolutely starving and need food ASAP!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer cubed ice or crushed ice?\")\n",
    "]\n",
    "chatgpt_response = get_response(client, list_of_prompts)\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that same prompt again, but modify only our system prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have a preference, but I can imagine enjoying the sound of cubed ice clinking in a glass on a warm summer day. How about you?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are joyful and having an awesome day!\"),\n",
    "    user_prompt(\"Do you prefer cubed ice or crushed ice?\")\n",
    "]\n",
    "\n",
    "chatgpt_response = get_response(client, list_of_prompts)\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9LHDPm0Na9ISXnY06nsiAclRgtkto', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I don't have a preference, but I can imagine enjoying the sound of cubed ice clinking in a glass on a warm summer day. How about you?\", role='assistant', function_call=None, tool_calls=None))], created=1714858091, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3b956da36b', usage=CompletionUsage(completion_tokens=33, prompt_tokens=30, total_tokens=63))\n"
     ]
    }
   ],
   "source": [
    "print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
    "\n",
    "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
    "\n",
    "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I picked a Stimple leaf from the Fallbean plant in the garden."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'Stumple' and 'Fallbean' in a sentence\")\n",
    "]\n",
    "\n",
    "simple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(simple_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
